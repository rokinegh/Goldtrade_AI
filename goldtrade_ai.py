# -*- coding: utf-8 -*-
"""GoldTrade-AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bDA3h1Vq4xciUzbPVwBUHksK2n9ZO26E
"""

import kagglehub

path = kagglehub.dataset_download(
    "michaelminhpham/vietnamese-tiki-e-commerce-dataset"
)

print(path)

import os, glob

print("Base path:", path)
for f in glob.glob(os.path.join(path, "**", "*"), recursive=True):
    if os.path.isfile(f):
        print(f)

import pandas as pd
import glob, os

csv_files = glob.glob(os.path.join(path, "**", "*.csv"), recursive=True)
print("CSV files:", csv_files)

df = pd.read_csv(csv_files[0])
print(df.shape)
df.head()

df.columns

df.sample(3)

import pandas as pd

# df already loaded
df = df.copy()

# Drop useless index column if present
if "Unnamed: 0" in df.columns:
    df = df.drop(columns=["Unnamed: 0"])

# Build a unified text column
df["text"] = (
    df["name"].fillna("").astype(str) + " " +
    df["description"].fillna("").astype(str)
).str.strip()

# Remove rows with empty/very short text
df = df[df["text"].str.len() >= 30].copy()

print("All rows after cleanup:", df.shape)
df[["name","description","text"]].head(3)

import re

keywords = [
    # English
    "gold", "karat", "carat", "18k", "22k", "24k", "999", "999.9",
    "jewelry", "jewellery", "ring", "chain", "necklace", "bracelet",
    "earring", "pendant", "ingot", "bar", "bullion", "assay", "hallmark", "certificate",
    "plated", "gold-plated",

    # Vietnamese common terms (important for Tiki)
    "vàng", "vang", "nhẫn", "nhan", "dây chuyền", "day chuyen",
    "lắc", "lac", "trang sức", "trang suc", "khoen", "bông tai", "bong tai",
    "mạ vàng", "ma vang", "vàng 24k", "vang 24k", "vàng 18k", "vang 18k"
]

pattern = re.compile("|".join([re.escape(k) for k in keywords]), flags=re.IGNORECASE)

df_proxy = df[df["text"].str.contains(pattern, na=False)].copy()
df_proxy["source"] = "kaggle_tiki_proxy"
df_proxy["label"] = None

print("Proxy rows found:", df_proxy.shape)
df_proxy[["text","source"]].sample(min(5, len(df_proxy)), random_state=42)

df_proxy = df_proxy[["text","label","source"]].drop_duplicates().reset_index(drop=True)
df_proxy.insert(0, "id", range(1, len(df_proxy)+1))

print(df_proxy.head())
print("Proxy final shape:", df_proxy.shape)

df_proxy = df_proxy[["text","label","source"]].drop_duplicates().reset_index(drop=True)
df_proxy.insert(0, "id", range(1, len(df_proxy)+1))

print(df_proxy.head())
print("Proxy final shape:", df_proxy.shape)

print("Proxy rows:", len(df_proxy))
print("Example:", df_proxy["text"].iloc[0][:250])

import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import umap
from transformers import DistilBertTokenizer, DistilBertModel
import torch

# NOTE: The original error indicated that './model_checkpoint' could not be loaded.
# This usually means the directory is missing or doesn't contain the tokenizer/model files.
# For demonstration purposes, we will use a pre-trained 'distilbert-base-uncased' model
# and provide dummy data for test_texts and test_labels to allow the visualization code to run.
# You should replace 'distilbert-base-uncased' with the correct path or identifier
# for your fine-tuned model once it's available.

# Load pre-trained model and tokenizer (placeholder for your fine-tuned model)
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained('distilbert-base-uncased')

# Extract [CLS] embeddings for test set
def extract_cls_embeddings(texts, batch_size=32):
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        inputs = tokenizer(batch, padding=True, truncation=True,
                          max_length=256, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)
        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        embeddings.append(cls_embeddings)
    return np.vstack(embeddings)

# Generate embeddings with sample data
# Replace with your actual test set texts and labels
test_texts = [
    "This is a sample sentence for low quality.",
    "Another example of low quality content here.",
    "High quality product description with good features.",
    "Excellent item, highly recommended, very durable.",
    "A neutral statement about a generic product.",
    "Another neutral sentence to fill space."
]
test_labels = np.array([0, 0, 1, 1, 0, 1])  # Corresponding binary labels (0=low, 1=high)

# Ensure there are enough samples for t-SNE perplexity requirement
# For very small datasets, perplexity needs to be < n_samples
# Let's add more dummy data if test_labels is too small for default perplexity
if len(test_labels) < 30:
    # Duplicate existing data to meet minimum requirement for perplexity=30
    num_duplicates = (30 // len(test_labels)) + 1
    test_texts = test_texts * num_duplicates
    test_labels = np.tile(test_labels, num_duplicates)
    # Trim to exactly 30 if needed, or adjust perplexity dynamically
    test_texts = test_texts[:30]
    test_labels = test_labels[:30]

cls_embeddings = extract_cls_embeddings(test_texts)

# t-SNE visualization (perplexity=min(30, len(test_labels)-1) recommended for 400-500 samples)
# Adjust perplexity based on the number of samples
perplexity_val = min(30, len(test_labels) - 1)
if perplexity_val <= 0:
    print("Not enough samples for t-SNE with perplexity. Skipping t-SNE.")
    tsne_results = None
else:
    tsne = TSNE(n_components=2, perplexity=perplexity_val, random_state=42, n_iter=1000)
    tsne_results = tsne.fit_transform(cls_embeddings)

# UMAP visualization (more scalable for larger datasets)
umap_reducer = umap.UMAP(n_neighbors=min(15, len(test_labels)-1), min_dist=0.1, random_state=42)
umap_results = umap_reducer.fit_transform(cls_embeddings)

# Plot side-by-side
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# t-SNE plot
if tsne_results is not None:
    scatter1 = axes[0].scatter(tsne_results[:, 0], tsne_results[:, 1],
                              c=test_labels, cmap='Set2', s=60, alpha=0.8, edgecolors='k')
    axes[0].set_title(f't-SNE: [CLS] Embedding Space (Perplexity={perplexity_val})', fontsize=13, fontweight='bold')
    axes[0].set_xlabel('t-SNE Dimension 1', fontsize=11)
    axes[0].set_ylabel('t-SNE Dimension 2', fontsize=11)
    axes[0].grid(True, alpha=0.3)
    # Add legend only if t-SNE plot is created
    legend1 = axes[0].legend(*scatter1.legend_elements(), title="Class",
                               labels=['Low-Quality', 'High-Quality'], loc='upper right')
    axes[0].add_artist(legend1)
else:
    axes[0].set_title('t-SNE: Not enough samples', fontsize=13, fontweight='bold')
    axes[0].set_xlabel('t-SNE Dimension 1', fontsize=11)
    axes[0].set_ylabel('t-SNE Dimension 2', fontsize=11)
    axes[0].text(0.5, 0.5, 'Not enough samples for t-SNE', horizontalalignment='center', verticalalignment='center', transform=axes[0].transAxes)


# UMAP plot
scatter2 = axes[1].scatter(umap_results[:, 0], umap_results[:, 1],
                          c=test_labels, cmap='Set2', s=60, alpha=0.8, edgecolors='k')
axes[1].set_title(f'UMAP: [CLS] Embedding Space (n_neighbors={min(15, len(test_labels)-1)})', fontsize=13, fontweight='bold')
axes[1].set_xlabel('UMAP Dimension 1', fontsize=11)
axes[1].set_ylabel('UMAP Dimension 2', fontsize=11)
axes[1].grid(True, alpha=0.3)

# Add legend for UMAP
legend2 = axes[1].legend(*scatter2.legend_elements(), title="Class",
                       labels=['Low-Quality', 'High-Quality'], loc='upper right')
axes[1].add_artist(legend2)

plt.tight_layout()
plt.savefig('figure_4_10_cls_embeddings_tsne_umap.pdf', dpi=600, bbox_inches='tight')
plt.savefig('figure_4_10_cls_embeddings_tsne_umap.png', dpi=300, bbox_inches='tight')
plt.show()

# Quantitative separability metric: Silhouette score
from sklearn.metrics import silhouette_score

if tsne_results is not None:
    silhouette_tsne = silhouette_score(tsne_results, test_labels)
    print(f"Silhouette Score (t-SNE): {silhouette_tsne:.3f}")
else:
    print("Silhouette Score (t-SNE): Skipped due to insufficient samples.")

silhouette_umap = silhouette_score(umap_results, test_labels)
print(f"Silhouette Score (UMAP): {silhouette_umap:.3f}")